{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjOIcZ/kMUw4aLaAXXjj+f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/blob/main/7_1_working_with_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Text Data\n",
        "This implementation covers both one-hot encoding and word embedding for movie reviews, demonstrating the transformation from raw text to numerical vectors.\n",
        "* Steps 1-5 involve preparing the data and converting text to one-hot encoded vectors, which are binary vectors representing each word.\n",
        "* Steps 6-7 involve initializing a word embedding layer and converting text to dense embedding vectors, which are numerical representations learned by the model.\n",
        "* Step 8 involves printing the original reviews and their corresponding vector representations to visualize the transformation."
      ],
      "metadata": {
        "id": "Dw5BUiTo4nJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6JVHqS2p07qC",
        "outputId": "8103cdd7-1b75-4b12-ae21-c283ae158434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch scikit-learn numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "I16V2hWH4w9z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Defines a list of sample movie reviews\n",
        "We start by creating a list of movie reviews. Each review is a string that represents a single user’s opinion about a movie. This list serves as our input data for the process."
      ],
      "metadata": {
        "id": "cp8J6YP64wFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample movie reviews\n",
        "reviews = [\n",
        "    \"I love this movie, it's amazing!\",\n",
        "    \"The movie was okay, not great.\",\n",
        "    \"I didn't like the movie at all.\",\n",
        "    \"Absolutely fantastic! A must-watch.\",\n",
        "    \"Not my type of movie, very boring.\"\n",
        "]"
      ],
      "metadata": {
        "id": "iYQO59uL45PQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Creates a vocabulary set from the reviews\n",
        "We build a vocabulary by extracting all unique words from the reviews. This vocabulary set will be used to encode the reviews."
      ],
      "metadata": {
        "id": "Rg471Ek847z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary set\n",
        "vocab = set(\" \".join(reviews).split())\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "knfjF1wi48br"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Maps each word in the vocabulary to a unique index\n",
        "We create a mapping from each word in the vocabulary to a unique integer index. This helps in converting words to numerical representations."
      ],
      "metadata": {
        "id": "JWrbMygp5EDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from word to index\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}"
      ],
      "metadata": {
        "id": "qmQGh1o45EUO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Uses the OneHotEncoder from sklearn.preprocessing to one-hot encode the words\n",
        "We use OneHotEncoder to encode each word as a one-hot vector. A one-hot vector is a binary vector with one “1” and all other elements “0”, corresponding to the unique index of the word in the vocabulary."
      ],
      "metadata": {
        "id": "IQ_yHZOy5L13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the words\n",
        "one_hot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "one_hot_encoder.fit(np.array(list(vocab)).reshape(-1, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "DLzP3HLW5MGD",
        "outputId": "d9244596-6c79-447b-d3b3-93fb4f2bc24e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(sparse=False, sparse_output=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(sparse=False, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(sparse=False, sparse_output=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Converts each review into one-hot encoded vectors\n",
        "For each review, we split the review into words and convert each word into its one-hot encoded vector using the encoder."
      ],
      "metadata": {
        "id": "xz5BAZkm5SiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert reviews to one-hot encoded vectors\n",
        "def review_to_one_hot_vectors(review):\n",
        "    words = review.split()\n",
        "    word_indices = [word_to_index[word] for word in words]\n",
        "    one_hot_vectors = one_hot_encoder.transform(np.array(words).reshape(-1, 1))\n",
        "    return one_hot_vectors"
      ],
      "metadata": {
        "id": "JVVCIUsi5SHZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Initializes a word embedding layer using torch.nn.Embedding\n",
        "We create a word embedding layer using PyTorch’s nn.Embedding. This layer will learn to map words to dense vectors (embeddings) of a specified dimension during the training of a neural network."
      ],
      "metadata": {
        "id": "1b2XhsPQ5ZFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding using PyTorch's nn.Embedding\n",
        "embedding_dim = 10  # Size of the word embedding vectors\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)"
      ],
      "metadata": {
        "id": "6hjOOx3R5Y6I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Converts each review into word embedding vectors\n",
        "We convert each review into embedding vectors by passing the index of each word through the embedding layer. The embedding layer transforms these indices into dense vectors."
      ],
      "metadata": {
        "id": "IsnoJC6q5eIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert reviews to word embedding vectors\n",
        "def review_to_embedding_vectors(review):\n",
        "    words = review.split()\n",
        "    word_indices = torch.tensor([word_to_index[word] for word in words], dtype=torch.long)\n",
        "    embedding_vectors = embedding(word_indices)\n",
        "    return embedding_vectors"
      ],
      "metadata": {
        "id": "CbfvDx5V5d5B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Prints the reviews and their corresponding one-hot encoded vectors and word embedding vectors\n",
        "Finally, we print the original reviews along with their one-hot encoded vectors and word embedding vectors to see the transformation from text to numerical representations."
      ],
      "metadata": {
        "id": "aK9iwxwo5lAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all reviews to one-hot and embedding vectors\n",
        "one_hot_review_vectors = [review_to_one_hot_vectors(review) for review in reviews]\n",
        "embedding_review_vectors = [review_to_embedding_vectors(review) for review in reviews]\n",
        "\n",
        "# Print the movie reviews and their one-hot encoding vectors\n",
        "for review, one_hot_vector in zip(reviews, one_hot_review_vectors):\n",
        "    print(f\"Review: {review}\")\n",
        "    print(\"One-hot Encoding Vectors:\")\n",
        "    print(one_hot_vector)\n",
        "    print()\n",
        "\n",
        "# Print the movie reviews and their embedding vectors\n",
        "for review, embedding_vector in zip(reviews, embedding_review_vectors):\n",
        "    print(f\"Review: {review}\")\n",
        "    print(\"Embedding Vectors:\")\n",
        "    print(embedding_vector)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeIKn0jD2D0Z",
        "outputId": "e5535f76-d349-44c8-ca19-38806f342edb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: I love this movie, it's amazing!\n",
            "One-hot Encoding Vectors:\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]]\n",
            "\n",
            "Review: The movie was okay, not great.\n",
            "One-hot Encoding Vectors:\n",
            "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]]\n",
            "\n",
            "Review: I didn't like the movie at all.\n",
            "One-hot Encoding Vectors:\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]]\n",
            "\n",
            "Review: Absolutely fantastic! A must-watch.\n",
            "One-hot Encoding Vectors:\n",
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]]\n",
            "\n",
            "Review: Not my type of movie, very boring.\n",
            "One-hot Encoding Vectors:\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]]\n",
            "\n",
            "Review: I love this movie, it's amazing!\n",
            "Embedding Vectors:\n",
            "tensor([[ 1.1189,  0.6298, -0.5967,  1.0846,  1.9306,  1.3600,  0.6887,  0.1950,\n",
            "         -0.7623,  1.1910],\n",
            "        [-0.4499,  0.6615, -0.2699,  0.6384,  0.9761, -3.2376, -1.0613, -1.7293,\n",
            "         -0.5904, -0.2527],\n",
            "        [ 0.5162,  1.5867,  0.8359, -0.3094,  1.1223,  0.2754, -0.1586,  0.6271,\n",
            "          1.0243,  0.5290],\n",
            "        [-1.3142, -0.1331,  0.1565,  0.2145, -1.6365,  0.6842, -0.4671, -0.2207,\n",
            "          0.3449,  1.3623],\n",
            "        [-0.6292,  0.8136, -0.1598,  1.2881,  1.3239,  1.3557, -1.6964,  0.9581,\n",
            "         -1.3927, -0.9109],\n",
            "        [-2.1501,  0.0535,  0.4377, -0.8261, -1.2107, -0.2431,  0.2840,  0.9815,\n",
            "          0.2476, -1.6642]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Review: The movie was okay, not great.\n",
            "Embedding Vectors:\n",
            "tensor([[ 9.1461e-01,  1.1239e+00, -2.8725e-01, -6.2479e-01, -3.8828e-01,\n",
            "          6.9818e-02,  6.1662e-01, -3.3846e-01,  1.5538e+00, -7.6521e-01],\n",
            "        [ 5.6371e-01,  1.7126e+00, -1.2237e-01,  2.0214e+00,  1.3567e+00,\n",
            "          2.0398e+00,  1.5240e+00, -4.0862e-01,  2.8584e+00, -1.2728e+00],\n",
            "        [ 1.0306e-01, -5.9564e-01, -5.1168e-01, -1.2551e+00, -4.5188e-04,\n",
            "         -2.0126e+00,  7.7233e-01,  8.1841e-01, -6.4698e-01,  2.6041e-01],\n",
            "        [-1.2006e-01,  1.0853e+00,  2.8066e-01,  2.2511e+00,  5.0074e-01,\n",
            "          6.0996e-01, -5.0629e-01, -1.2116e-01, -1.5223e-01,  1.2682e+00],\n",
            "        [-2.5611e-01, -1.2519e-02, -1.6186e+00,  1.2196e+00, -7.7264e-01,\n",
            "          5.9471e-01,  7.2896e-01, -2.9596e-01,  9.6950e-01, -1.0065e+00],\n",
            "        [-8.1522e-01,  1.5379e-01,  7.9180e-01, -1.1252e+00,  5.1588e-02,\n",
            "          2.4003e-01,  5.4092e-01, -3.7115e-01, -2.6245e-01,  5.5553e-01]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Review: I didn't like the movie at all.\n",
            "Embedding Vectors:\n",
            "tensor([[ 1.1189,  0.6298, -0.5967,  1.0846,  1.9306,  1.3600,  0.6887,  0.1950,\n",
            "         -0.7623,  1.1910],\n",
            "        [-2.1528, -1.4436, -0.2280,  0.3267,  0.2509, -0.5719, -0.1259,  1.8505,\n",
            "          0.8263, -0.9535],\n",
            "        [ 0.0408, -1.0116, -0.8320,  0.2526, -0.8246,  0.5927,  0.6143,  1.1094,\n",
            "         -0.1126,  0.6464],\n",
            "        [ 0.1810,  0.5882,  0.4489,  1.5377, -1.1443,  0.7875, -0.7661,  1.2435,\n",
            "         -1.6655, -1.2172],\n",
            "        [ 0.5637,  1.7126, -0.1224,  2.0214,  1.3567,  2.0398,  1.5240, -0.4086,\n",
            "          2.8584, -1.2728],\n",
            "        [-0.9633, -0.7594,  0.0227,  0.0859,  1.4720, -0.4089,  0.1008, -0.8236,\n",
            "          0.3900,  0.1976],\n",
            "        [-0.8895, -0.9560,  0.4674, -0.3073, -0.1154,  1.8773, -1.2190, -0.0413,\n",
            "          0.8031, -0.9286]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Review: Absolutely fantastic! A must-watch.\n",
            "Embedding Vectors:\n",
            "tensor([[-0.2698,  1.4778, -0.3759,  3.2977, -1.0910,  1.4463,  1.2798, -0.0088,\n",
            "          0.1278,  0.7528],\n",
            "        [ 0.5005, -1.1584,  1.2082, -1.0741, -0.4484, -1.3328,  0.3114,  0.2554,\n",
            "          1.2079, -0.6710],\n",
            "        [-0.5047, -0.2969, -2.3654, -0.2823,  0.1981, -1.4437,  0.6402,  0.5344,\n",
            "          1.1659, -0.0236],\n",
            "        [ 0.0175, -2.0027,  1.4722, -0.9639,  1.1748,  0.3753, -2.1888,  0.5052,\n",
            "          2.5630, -0.4906]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Review: Not my type of movie, very boring.\n",
            "Embedding Vectors:\n",
            "tensor([[ 0.9588, -1.8593,  0.0220, -0.0390, -0.0543,  0.3966,  0.8013,  0.9498,\n",
            "         -1.9848, -1.1730],\n",
            "        [-0.9825, -1.0422,  0.5574,  1.0440,  0.8967,  1.1919,  0.6711,  2.4899,\n",
            "          0.9018, -0.7936],\n",
            "        [-0.2139,  1.6384, -0.8532,  0.2227,  0.8611, -0.8372, -0.2682, -0.2911,\n",
            "         -1.2646, -0.0683],\n",
            "        [-0.6338,  0.1749, -0.2420, -0.7569,  1.5285,  0.8148, -0.5116, -1.2999,\n",
            "         -0.1513,  0.5379],\n",
            "        [-1.3142, -0.1331,  0.1565,  0.2145, -1.6365,  0.6842, -0.4671, -0.2207,\n",
            "          0.3449,  1.3623],\n",
            "        [-1.0472,  0.2475, -0.6326, -0.7375, -1.9507,  0.7413, -0.0377,  1.1322,\n",
            "          0.1325, -2.2755],\n",
            "        [-0.3315,  1.1283,  0.6432, -0.7789, -0.3971,  1.4443, -1.4828,  0.7239,\n",
            "         -0.9423,  1.3943]], grad_fn=<EmbeddingBackward0>)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}