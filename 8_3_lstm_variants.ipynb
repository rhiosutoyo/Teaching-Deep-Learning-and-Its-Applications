{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/gRqtBCC5X29Vt08LLxky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/blob/main/8_3_lstm_variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Variants: Peephole Connections, GRU, and biLSTM\n",
        "\n",
        "## Definition\n",
        "LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) that are designed to better capture long-term dependencies in sequential data.\n"
      ],
      "metadata": {
        "id": "_N0pN9VyeEvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZhUuW-Gbf3l_",
        "outputId": "40e5c52b-933b-4541-fcf2-be5d34d5931f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "bZiGLeshgNxd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some random data\n",
        "batch_size = 32\n",
        "seq_length = 10\n",
        "input_dim = 8\n",
        "hidden_dim = 16\n",
        "\n",
        "x = torch.randn(batch_size, seq_length, input_dim)"
      ],
      "metadata": {
        "id": "GxMxaqHLhm7y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard LSTM\n",
        "class StandardLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(StandardLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "w2zV92XrgREM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Variants\n",
        "There are several variants of LSTM, each introducing modifications to improve performance or adapt to specific tasks.\n",
        "1.   **Peephole Connections**: Enhance LSTM by allowing gates to access cell state, improving learning in timing tasks.\n",
        "2.   **GRU (Gated Recurrent Unit)**: Simplified LSTM variant with combined cell and hidden states, and only two gates (reset and update).\n",
        "3. **biLSTM (Bidirectional LSTM)**: Processes sequences in both forward and backward directions, leveraging past and future context."
      ],
      "metadata": {
        "id": "HHtQEn8Cfz4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Peephole Connections\n",
        "\n",
        "## Definition\n",
        "Peephole connections are an enhancement to the traditional LSTM architecture. In a standard LSTM, the gates (input gate, forget gate, and output gate) are controlled by the current input and the previous hidden state. Peephole connections add connections from the cell state to the gates, allowing the gates to also access the cell state. This can improve the LSTM’s ability to learn timing tasks where the precise intervals are important."
      ],
      "metadata": {
        "id": "O8mPoMOoexA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Peephole LSTM\n",
        "class PeepholeLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(PeepholeLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.W_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
        "        self.U_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.C_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
        "        self.U_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.C_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
        "        self.U_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.C_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
        "\n",
        "        self.W_c = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
        "        self.U_c = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.W_i)\n",
        "        nn.init.xavier_uniform_(self.U_i)\n",
        "        nn.init.zeros_(self.C_i)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_f)\n",
        "        nn.init.xavier_uniform_(self.U_f)\n",
        "        nn.init.zeros_(self.C_f)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_o)\n",
        "        nn.init.xavier_uniform_(self.U_o)\n",
        "        nn.init.zeros_(self.C_o)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_c)\n",
        "        nn.init.xavier_uniform_(self.U_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_t = torch.zeros(x.size(0), self.hidden_dim).to(x.device)\n",
        "        c_t = torch.zeros(x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(x.size(1)):\n",
        "            x_t = x[:, t, :]\n",
        "            i_t = torch.sigmoid(x_t @ self.W_i + h_t @ self.U_i + c_t * self.C_i)\n",
        "            f_t = torch.sigmoid(x_t @ self.W_f + h_t @ self.U_f + c_t * self.C_f)\n",
        "            o_t = torch.sigmoid(x_t @ self.W_o + h_t @ self.U_o + c_t * self.C_o)\n",
        "            c_hat_t = torch.tanh(x_t @ self.W_c + h_t @ self.U_c)\n",
        "            c_t = f_t * c_t + i_t * c_hat_t\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            outputs.append(h_t.unsqueeze(1))\n",
        "        return torch.cat(outputs, dim=1)"
      ],
      "metadata": {
        "id": "xsUCQjUkf4xM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. GRU (Gated Recurrent Unit)\n",
        "\n",
        "## Definition\n",
        "The GRU is another type of RNN designed to solve the vanishing gradient problem and efficiently capture long-term dependencies, similar to LSTM but with a simpler architecture. GRUs combine the cell state and hidden state and have only two gates: a reset gate and an update gate."
      ],
      "metadata": {
        "id": "FnKK5GNGfE38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TuVO_yhbf5ZF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. biLSTM (Bidirectional LSTM)\n",
        "\n",
        "## Definition\n",
        "A biLSTM processes the input data in both forward and backward directions, allowing the network to have information from both past and future contexts. This is particularly useful for tasks where context from both directions is important, such as language modeling or sequence tagging."
      ],
      "metadata": {
        "id": "43XgW0AufNDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.bilstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.bilstm(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "M4uUOKczf6B3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate and run the models"
      ],
      "metadata": {
        "id": "L7cAO1jagbda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 8\n",
        "hidden_dim = 16\n",
        "\n",
        "standard_lstm = StandardLSTM(input_dim, hidden_dim)\n",
        "peephole_lstm = PeepholeLSTM(input_dim, hidden_dim)\n",
        "gru = GRU(input_dim, hidden_dim)\n",
        "bilstm = BiLSTM(input_dim, hidden_dim)\n",
        "\n",
        "x = torch.randn(batch_size, seq_length, input_dim)\n",
        "\n",
        "print(\"Standard LSTM output shape:\", standard_lstm(x).shape)\n",
        "print(\"Peephole LSTM output shape:\", peephole_lstm(x).shape)\n",
        "print(\"GRU output shape:\", gru(x).shape)\n",
        "print(\"BiLSTM output shape:\", bilstm(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmopCplhgaEJ",
        "outputId": "ab1b9228-d9b9-4c09-fe0f-c8733799342f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard LSTM output shape: torch.Size([32, 10, 16])\n",
            "Peephole LSTM output shape: torch.Size([32, 10, 16])\n",
            "GRU output shape: torch.Size([32, 10, 16])\n",
            "BiLSTM output shape: torch.Size([32, 10, 32])\n"
          ]
        }
      ]
    }
  ]
}