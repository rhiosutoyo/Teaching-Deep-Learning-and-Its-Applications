{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtXwwxGJQw3ymAZK3nRWwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/blob/main/7_2_training_word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Word Embedding\n",
        "\n",
        "## Definition\n",
        "Training word embeddings involves learning a vector representation for each word in a vocabulary such that words with similar meanings have similar representations. This process is often done using neural networks and relies on context words to inform the embeddings.\n",
        "\n",
        "## Code Implementation\n",
        "The code sets up and trains a word embedding model using the Skip-gram approach. The embeddings capture the semantic meaning of words based on their context in the training corpus. This is achieved through a series of steps involving data preparation, model definition, and iterative training."
      ],
      "metadata": {
        "id": "pU7UwWqp_v1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XV69X7Sx-XeA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocesses the text data to create a vocabulary and tokenizes the sentences\n",
        "\n",
        "The code starts with a corpus of sentences and tokenizes them to build a vocabulary. Each word is assigned a unique index.\n",
        "\n",
        "* **Tokenization**: The tokenize function converts each sentence into a list of lowercase words.\n",
        "* **Building the Vocabulary**: The build_vocab function constructs a vocabulary from the tokenized sentences. It assigns a unique index to each word based on its frequency in the corpus. This vocabulary is used to convert words into numerical indices."
      ],
      "metadata": {
        "id": "Xf5fJ49B_8bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "corpus = [\n",
        "    \"I love programming\",\n",
        "    \"Programming is fun\",\n",
        "    \"I love learning new things\",\n",
        "    \"Deep learning is a subset of machine learning\",\n",
        "    \"PyTorch is a great library for deep learning\",\n",
        "    \"Machine learning and deep learning are fascinating\",\n",
        "    \"Natural language processing is a part of AI\",\n",
        "    \"AI is transforming the world\",\n",
        "    \"We can build intelligent systems using AI\",\n",
        "    \"Programming requires logical thinking\"\n",
        "]"
      ],
      "metadata": {
        "id": "1wznSAFJ-U5f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def build_vocab(corpus):\n",
        "    tokens = [tokenize(sentence) for sentence in corpus]\n",
        "    counter = Counter([token for sentence in tokens for token in sentence])\n",
        "    vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(corpus)\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "ujVgzwyp-q33"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Defines a TextDataset class to handle the context words for training\n",
        "\n",
        "The TextDataset class generates pairs of center and context words. This means for each word in a sentence (center word), it considers nearby words (context words) within a defined window size.\n",
        "\n",
        "* **Initialization**: The TextDataset class takes the corpus and vocabulary as inputs and generates pairs of center and context words. It considers a context window size to determine the context words around each center word.\n",
        "\n",
        "* **Data Storage**: The dataset stores these pairs as tuples of indices, representing the center and context words.\n",
        "\n",
        "* **Dataset Methods**: The __len__ method returns the number of pairs, and the __getitem__ method retrieves a specific pair, converting them to tensors."
      ],
      "metadata": {
        "id": "-958PHId-16X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, corpus, vocab, context_size=2):\n",
        "        self.data = []\n",
        "        self.vocab = vocab\n",
        "        for sentence in corpus:\n",
        "            tokens = tokenize(sentence)\n",
        "            indices = [vocab[token] for token in tokens]\n",
        "            for center_pos in range(len(indices)):\n",
        "                for context_pos in range(-context_size, context_size + 1):\n",
        "                    if context_pos == 0 or center_pos + context_pos < 0 or center_pos + context_pos >= len(indices):\n",
        "                        continue\n",
        "                    context_word = indices[center_pos + context_pos]\n",
        "                    self.data.append((indices[center_pos], context_word))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center_word, context_word = self.data[idx]\n",
        "        return torch.tensor(center_word, dtype=torch.long), torch.tensor(context_word, dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(corpus, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "dAdmf7oO-vqQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Defines a simple word embedding model using PyTorchâ€™s nn.Embedding\n",
        "\n",
        "The WordEmbeddingModel class uses an embedding layer (nn.Embedding). This layer learns a fixed-size vector (embedding) for each word in the vocabulary.\n",
        "\n",
        "* **Model Definition**: The WordEmbeddingModel class initializes an embedding layer with the vocabulary size and embedding dimensions.\n",
        "\n",
        "* **Forward Method**: The forward method returns the embeddings for the given center words, which are looked up in the embedding layer."
      ],
      "metadata": {
        "id": "hR3YyrmB_A3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class WordEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(WordEmbeddingModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, center_words):\n",
        "        return self.embeddings(center_words)\n",
        "\n",
        "embed_size = 10\n",
        "model = WordEmbeddingModel(vocab_size, embed_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0cPHCLeD_Avp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Trains the model using a basic context-based approach\n",
        "\n",
        "During training, the model learns embeddings by predicting context words given a center word. This is done by:\n",
        "* Forward Pass: Looking up the embeddings for the center words.\n",
        "* Context Prediction: Using these embeddings to predict the indices of the context words. The torch.matmul(embeddings, model.embeddings.weight.t()) computes scores (similarities) for all words in the vocabulary.\n",
        "* Loss Calculation: Using CrossEntropyLoss, which compares these scores with the actual context word indices.\n",
        "* Backward Pass: Computing gradients and updating the model parameters to minimize the loss."
      ],
      "metadata": {
        "id": "gKEf0Z_6_J6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for center_words, context_words in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(center_words)\n",
        "        outputs = torch.matmul(embeddings, model.embeddings.weight.t())\n",
        "        loss = criterion(outputs, context_words)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuKSqYzW_Jya",
        "outputId": "dd30c600-410a-48d3-81b4-679b797331d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 326.0805\n",
            "Epoch [20/100], Loss: 267.9783\n",
            "Epoch [30/100], Loss: 226.5356\n",
            "Epoch [40/100], Loss: 196.5373\n",
            "Epoch [50/100], Loss: 174.2668\n",
            "Epoch [60/100], Loss: 157.4672\n",
            "Epoch [70/100], Loss: 144.4252\n",
            "Epoch [80/100], Loss: 134.2227\n",
            "Epoch [90/100], Loss: 126.3131\n",
            "Epoch [100/100], Loss: 120.1455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Tests the model with 10 sample sentences and prints their embeddings\n",
        "\n",
        "After training, the model can generate embeddings for new sentences. The embeddings for each word can be used in various downstream tasks, such as similarity measurement or as input features for other models.\n",
        "\n",
        "* **Tokenization and Indexing**: Each test sentence is tokenized, and the words are converted to their corresponding indices using the vocabulary.\n",
        "* **Embedding Lookup**: The model generates embeddings for the words in each test sentence.\n",
        "* **Output**: The embeddings for each word in the test sentences are printed, showing the learned representations."
      ],
      "metadata": {
        "id": "gdTwDzOS_W3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warning\n",
        "This is just example code. For production use, consider improvements and optimizations\n",
        "\n",
        "Steps to improve accuracy and reduce loss:\n",
        "1. Increase the size of the corpus: Use a larger and more diverse set of text data to train the model.\n",
        "2. Use negative sampling: Implement negative sampling to reduce computational complexity and improve training efficiency.\n",
        "3. Tune hyperparameters: Experiment with different embedding sizes, learning rates, and context window sizes to find the optimal settings for your specific dataset."
      ],
      "metadata": {
        "id": "paO3puTqCP0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrhlX6Ef7kt4",
        "outputId": "56252bcc-cbd7-4725-a804-2f061b68c650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"AI is amazing\"\n",
            "Embeddings: [[-0.12648243 -1.4020289  -0.8643976  -0.12619703 -0.04087214  0.73555386\n",
            "   0.72107124 -0.377158    0.24512364  0.35317817]\n",
            " [ 0.70755345  0.4047808   0.16402324  0.0699135   0.16389346  0.5835555\n",
            "   0.9639186  -0.65452385 -0.07893892  0.89540154]]\n",
            "\n",
            "Sentence: \"Learning is a continuous process\"\n",
            "Embeddings: [[-0.02035596  0.07210164  0.69371    -0.03727553  1.5827276   0.27384332\n",
            "   0.44894344 -0.04693383  0.10514028 -0.9143499 ]\n",
            " [ 0.70755345  0.4047808   0.16402324  0.0699135   0.16389346  0.5835555\n",
            "   0.9639186  -0.65452385 -0.07893892  0.89540154]\n",
            " [ 0.4678213   0.40463117  0.8982228  -0.0554723   0.22818246  0.7663856\n",
            "   1.0892923  -0.4460114  -0.31314376  0.64136964]]\n",
            "\n",
            "Sentence: \"Programming opens many doors\"\n",
            "Embeddings: [[-0.80301255  0.5380913  -0.44144842  0.5302582  -0.3307413   0.7893749\n",
            "  -0.02127253 -0.44663918  0.83124155  0.9281888 ]]\n",
            "\n",
            "Sentence: \"Deep learning uses neural networks\"\n",
            "Embeddings: [[ 0.02173646  0.23279826  0.26432922  0.07395178  2.2978866   0.09796233\n",
            "   0.37071064  0.36056256 -0.08808693 -0.0583855 ]\n",
            " [-0.02035596  0.07210164  0.69371    -0.03727553  1.5827276   0.27384332\n",
            "   0.44894344 -0.04693383  0.10514028 -0.9143499 ]]\n",
            "\n",
            "Sentence: \"Data science is a growing field\"\n",
            "Embeddings: [[ 0.70755345  0.4047808   0.16402324  0.0699135   0.16389346  0.5835555\n",
            "   0.9639186  -0.65452385 -0.07893892  0.89540154]\n",
            " [ 0.4678213   0.40463117  0.8982228  -0.0554723   0.22818246  0.7663856\n",
            "   1.0892923  -0.4460114  -0.31314376  0.64136964]]\n",
            "\n",
            "Sentence: \"PyTorch makes neural network modeling easier\"\n",
            "Embeddings: [[ 0.40888536  0.28784665  0.79730177 -0.27465874 -0.01905205  0.74259686\n",
            "  -0.20349437 -0.8947189  -0.21249051  0.6433951 ]]\n",
            "\n",
            "Sentence: \"Machine learning is a core component of AI\"\n",
            "Embeddings: [[ 0.63616616 -0.286119    0.7016299  -0.3549384   0.79778296 -0.21958031\n",
            "   0.7919199   0.23758179  0.74868155 -0.86620957]\n",
            " [-0.02035596  0.07210164  0.69371    -0.03727553  1.5827276   0.27384332\n",
            "   0.44894344 -0.04693383  0.10514028 -0.9143499 ]\n",
            " [ 0.70755345  0.4047808   0.16402324  0.0699135   0.16389346  0.5835555\n",
            "   0.9639186  -0.65452385 -0.07893892  0.89540154]\n",
            " [ 0.4678213   0.40463117  0.8982228  -0.0554723   0.22818246  0.7663856\n",
            "   1.0892923  -0.4460114  -0.31314376  0.64136964]\n",
            " [ 0.06748268 -0.07794224  0.8016988  -0.45212182  0.37691996  0.99689376\n",
            "   1.1032873  -0.3211662   0.29176345 -0.19383624]\n",
            " [-0.12648243 -1.4020289  -0.8643976  -0.12619703 -0.04087214  0.73555386\n",
            "   0.72107124 -0.377158    0.24512364  0.35317817]]\n",
            "\n",
            "Sentence: \"Understanding AI is important\"\n",
            "Embeddings: [[-0.12648243 -1.4020289  -0.8643976  -0.12619703 -0.04087214  0.73555386\n",
            "   0.72107124 -0.377158    0.24512364  0.35317817]\n",
            " [ 0.70755345  0.4047808   0.16402324  0.0699135   0.16389346  0.5835555\n",
            "   0.9639186  -0.65452385 -0.07893892  0.89540154]]\n",
            "\n",
            "Sentence: \"Technology evolves rapidly\"\n",
            "Embeddings: []\n",
            "\n",
            "Sentence: \"Natural language processing enables communication with machines\"\n",
            "Embeddings: [[-1.23644713e-02  6.48481622e-02 -1.89608467e+00  7.17767000e-01\n",
            "  -4.45609331e-01 -7.46272743e-01  7.71356583e-01 -6.95399474e-04\n",
            "  -8.65553856e-01  6.49357378e-01]\n",
            " [ 5.25051892e-01 -3.27403575e-01 -4.85254019e-01  2.47712970e-01\n",
            "  -5.62439501e-01 -1.50807589e-01  1.19124448e+00 -3.55690658e-01\n",
            "  -1.04057395e+00 -6.61575645e-02]\n",
            " [ 5.82140625e-01  1.45553470e-01 -5.65437078e-01 -1.11205176e-01\n",
            "  -3.29535097e-01  5.32738753e-02  1.30351174e+00 -1.32045776e-01\n",
            "  -1.06809199e+00  3.70033115e-01]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "test_data = [\n",
        "    \"AI is amazing\",\n",
        "    \"Learning is a continuous process\",\n",
        "    \"Programming opens many doors\",\n",
        "    \"Deep learning uses neural networks\",\n",
        "    \"Data science is a growing field\",\n",
        "    \"PyTorch makes neural network modeling easier\",\n",
        "    \"Machine learning is a core component of AI\",\n",
        "    \"Understanding AI is important\",\n",
        "    \"Technology evolves rapidly\",\n",
        "    \"Natural language processing enables communication with machines\"\n",
        "]\n",
        "\n",
        "# Convert test data to embeddings\n",
        "for sentence in test_data:\n",
        "    tokens = tokenize(sentence)\n",
        "    indices = [vocab[token] for token in tokens if token in vocab]\n",
        "    embeddings = model(torch.tensor(indices, dtype=torch.long))\n",
        "    print(f'Sentence: \"{sentence}\"')\n",
        "    print('Embeddings:', embeddings.detach().numpy())\n",
        "    print()"
      ]
    }
  ]
}