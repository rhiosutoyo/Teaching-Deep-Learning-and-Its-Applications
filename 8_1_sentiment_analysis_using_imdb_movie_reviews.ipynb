{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyManxBYf6Xx4BsoU7JlmoJR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/blob/main/8_1_sentiment_analysis_using_imdb_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install torch torchvision torchtext spacy torchdata portalocker>=2.0.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nDzFdsN5L3OT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import torch\n",
        "import torchdata\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yW0ND8hATeuV",
        "outputId": "8e95aba0-74f0-4136-b6d6-ae364a9cbb1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Set the Seed for Reproducibility\n",
        "SEED = 88\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "id": "tLGB4rgwTk_S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load and Preprocess the Data\n",
        "def download_imdb_dataset():\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    filename = \"aclImdb_v1.tar.gz\"\n",
        "    dataset_folder = \"aclImdb\"\n",
        "\n",
        "    if not os.path.exists(dataset_folder):\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        with tarfile.open(filename, \"r:gz\") as tar:\n",
        "            tar.extractall()\n",
        "        os.remove(filename)\n",
        "\n",
        "download_imdb_dataset()\n",
        "\n",
        "def read_imdb_split(split):\n",
        "    split_path = f'aclImdb/{split}'\n",
        "    texts, labels = [], []\n",
        "    for label in ['pos', 'neg']:\n",
        "        dir_path = f'{split_path}/{label}'\n",
        "        for fname in os.listdir(dir_path):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(dir_path, fname), 'r', encoding='utf-8') as f:\n",
        "                    texts.append(f.read())\n",
        "                    labels.append(label)\n",
        "    return list(zip(labels, texts))\n",
        "\n",
        "train_data = read_imdb_split('train')\n",
        "test_data = read_imdb_split('test')\n",
        "train_data, valid_data = random_split(train_data, [20000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "-1oatGKWTkZw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define Text and Label Processing Functions\n",
        "def text_pipeline(x):\n",
        "    return [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "def label_pipeline(x):\n",
        "    return 1 if x == 'pos' else 0"
      ],
      "metadata": {
        "id": "yvBrJSxOTkPb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Create DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
        "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
        "    return label_list.to(device), text_list.to(device), lengths.to(device)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "yfFxghH2TkH7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Build the Model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "PGUa3GawTj8t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Initialize the Model\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = vocab['<pad>']\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS,\n",
        "            BIDIRECTIONAL, DROPOUT, PAD_IDX)"
      ],
      "metadata": {
        "id": "4uMi_SxpTyFy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Train the Model\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "kd72S29zTyDU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Training Loop\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        labels, text, text_lengths = batch\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            labels, text, text_lengths = batch\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ChPrrHTVTyAw",
        "outputId": "8735164d-938d-47a2-8fa0-ff5e88a0c72d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.682 | Train Acc: 56.37%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 55.20%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.654 | Train Acc: 60.81%\n",
            "\t Val. Loss: 0.638 |  Val. Acc: 60.70%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.603 | Train Acc: 67.15%\n",
            "\t Val. Loss: 0.537 |  Val. Acc: 73.32%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.496 | Train Acc: 76.48%\n",
            "\t Val. Loss: 0.408 |  Val. Acc: 81.61%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.433 | Train Acc: 80.29%\n",
            "\t Val. Loss: 0.389 |  Val. Acc: 82.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Evaluate on Test Data\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oHUhv8SMTx-b",
        "outputId": "564a8a19-da3c-499f-fafc-4003caac17db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.400 | Test Acc: 82.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Test with New Sentences\n",
        "test_reviews = [\n",
        "    \"The movie was fantastic! I really enjoyed it.\",\n",
        "    \"Absolutely terrible. Worst movie I've seen in years.\",\n",
        "    \"It was an okay movie. Nothing special, but not bad either.\",\n",
        "    \"I loved the acting and the storyline. Highly recommend!\",\n",
        "    \"The plot was very predictable and boring.\",\n",
        "    \"A masterpiece of cinema. Truly inspiring and well-made.\",\n",
        "    \"I didn't like the film at all. The characters were flat and uninteresting.\",\n",
        "    \"Great visuals, but the story lacked depth.\",\n",
        "    \"An excellent film with a powerful message.\",\n",
        "    \"Not my cup of tea. I found it quite dull and unengaging.\"\n",
        "]\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokens = text_pipeline(sentence)\n",
        "    text_lengths = torch.tensor([len(tokens)])\n",
        "    text_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "    text_lengths = text_lengths.to(device)\n",
        "    prediction = torch.sigmoid(model(text_tensor, text_lengths))\n",
        "    return prediction.item()\n",
        "\n",
        "for review in test_reviews:\n",
        "    sentiment = predict_sentiment(model, review)\n",
        "    label = 'positive' if sentiment >= 0.5 else 'negative'\n",
        "    print(f'Review: {review}\\nSentiment Score: {sentiment:.4f} ({label})\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7rt4K0QzTx4n",
        "outputId": "c7616a97-8ce2-4a7e-98a2-62542b2f95aa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: The movie was fantastic! I really enjoyed it.\n",
            "Sentiment Score: 0.7823 (positive)\n",
            "\n",
            "Review: Absolutely terrible. Worst movie I've seen in years.\n",
            "Sentiment Score: 0.0620 (negative)\n",
            "\n",
            "Review: It was an okay movie. Nothing special, but not bad either.\n",
            "Sentiment Score: 0.0143 (negative)\n",
            "\n",
            "Review: I loved the acting and the storyline. Highly recommend!\n",
            "Sentiment Score: 0.8376 (positive)\n",
            "\n",
            "Review: The plot was very predictable and boring.\n",
            "Sentiment Score: 0.0231 (negative)\n",
            "\n",
            "Review: A masterpiece of cinema. Truly inspiring and well-made.\n",
            "Sentiment Score: 0.8245 (positive)\n",
            "\n",
            "Review: I didn't like the film at all. The characters were flat and uninteresting.\n",
            "Sentiment Score: 0.0378 (negative)\n",
            "\n",
            "Review: Great visuals, but the story lacked depth.\n",
            "Sentiment Score: 0.8064 (positive)\n",
            "\n",
            "Review: An excellent film with a powerful message.\n",
            "Sentiment Score: 0.9218 (positive)\n",
            "\n",
            "Review: Not my cup of tea. I found it quite dull and unengaging.\n",
            "Sentiment Score: 0.4882 (negative)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}