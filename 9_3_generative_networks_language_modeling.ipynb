{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyziPmrubsRFkM58SGt2z4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/blob/main/9_3_generative_networks_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Networks: Language Modeling\n",
        "\n",
        "This Python code trains a word-level language model using an LSTM in PyTorch. It begins by downloading and preprocessing a text dataset, converting words to integers for model training. The `TextDataset` class creates input-target pairs for the model. In this example, we utilize Cinderella Story for the dataset.\n",
        "\n",
        "Then, the LSTM model is defined with embedding, LSTM, and fully connected layers. The training loop optimizes the model over multiple epochs using cross-entropy loss. A text generation function is provided, ensuring the starting words are in the vocauary, and generating new sequences based on the trained model.\n",
        "\n",
        "Lastly, the vocabulary is printed for verification."
      ],
      "metadata": {
        "id": "IST67WMASoEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "# Download the text data\n",
        "url = \"https://raw.githubusercontent.com/rhiosutoyo/Teaching-Deep-Learning-and-Its-Applications/main/dataset/cinderella-story.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Preprocessing\n",
        "words = text.split()\n",
        "vocab = Counter(words)\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "encoded_text = [word_to_idx[word] for word in words]\n",
        "\n",
        "# Parameters\n",
        "sequence_length = 4\n",
        "batch_size = 2\n",
        "\n",
        "# Prepare the dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, sequence_length):\n",
        "        self.text = text\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.text[idx:idx + self.sequence_length]),\n",
        "            torch.tensor(self.text[idx + 1:idx + self.sequence_length + 1]),\n",
        "        )\n",
        "\n",
        "dataset = TextDataset(encoded_text, sequence_length)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the LSTM-based language model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new_zeros(num_layers, batch_size, hidden_dim),\n",
        "                weight.new_zeros(num_layers, batch_size, hidden_dim))\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 10\n",
        "hidden_dim = 50\n",
        "num_layers = 2\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        # Initialize hidden state with batch size\n",
        "        hidden = model.init_hidden(inputs.size(0))\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Detach hidden state to prevent backpropagating through the entire training history\n",
        "        hidden = tuple([each.detach() for each in hidden])\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdR_OhvqPyUr",
        "outputId": "097469fc-4d4e-47cd-dc7a-af973d032f87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/200], Loss: 3.4668\n",
            "Epoch [10/200], Loss: 1.3209\n",
            "Epoch [20/200], Loss: 0.5407\n",
            "Epoch [30/200], Loss: 1.4255\n",
            "Epoch [40/200], Loss: 0.4448\n",
            "Epoch [50/200], Loss: 0.6190\n",
            "Epoch [60/200], Loss: 0.4712\n",
            "Epoch [70/200], Loss: 0.6780\n",
            "Epoch [80/200], Loss: 2.1376\n",
            "Epoch [90/200], Loss: 0.0535\n",
            "Epoch [100/200], Loss: 0.9925\n",
            "Epoch [110/200], Loss: 1.2489\n",
            "Epoch [120/200], Loss: 0.6730\n",
            "Epoch [130/200], Loss: 0.0051\n",
            "Epoch [140/200], Loss: 0.7394\n",
            "Epoch [150/200], Loss: 0.9580\n",
            "Epoch [160/200], Loss: 0.0171\n",
            "Epoch [170/200], Loss: 0.0275\n",
            "Epoch [180/200], Loss: 0.9480\n",
            "Epoch [190/200], Loss: 1.0516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation function\n",
        "def generate_text(model, start_text, length=20):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_hidden(1)\n",
        "\n",
        "    for _ in range(length):\n",
        "        x = torch.tensor([[word_to_idx[w] for w in words[-sequence_length:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_idx = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(idx_to_word[word_idx])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Generate text with a default start text that is in the vocabulary\n",
        "start_text = \"Cinderella was\"\n",
        "print(generate_text(model, start_text))\n",
        "\n",
        "start_text = \"Cinderella was\"\n",
        "print(generate_text(model, start_text))\n",
        "\n",
        "start_text = \"Cinderella was\"\n",
        "print(generate_text(model, start_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXN2oUIBR4kR",
        "outputId": "4ab139b8-1164-46ce-ed61-a2e860276788"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cinderella was the happiest she had ever been. But as the hours would the filled the room. It was a time, in\n",
            "Cinderella was reunited with the prince, and they lived happily ever after, proving that kindness was the most important thing in the\n",
            "Cinderella was the happiest she had ever been. But as the hours would the filled the room. It was a kind, gentle,\n"
          ]
        }
      ]
    }
  ]
}